{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este arquivo é utilizadp para treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 468 images belonging to 7 classes.\n",
      "Found 36 images belonging to 7 classes.\n",
      "Epoch 1/25\n",
      "15/15 [==============================] - 8s 415ms/step - loss: 1.4161 - accuracy: 0.4017 - val_loss: 1.1122 - val_accuracy: 0.3056\n",
      "Epoch 2/25\n",
      "15/15 [==============================] - 7s 446ms/step - loss: 0.9665 - accuracy: 0.5534 - val_loss: 1.4835 - val_accuracy: 0.3611\n",
      "Epoch 3/25\n",
      "15/15 [==============================] - 9s 575ms/step - loss: 0.7933 - accuracy: 0.6688 - val_loss: 1.6318 - val_accuracy: 0.4167\n",
      "Epoch 4/25\n",
      "15/15 [==============================] - 10s 659ms/step - loss: 0.7055 - accuracy: 0.7500 - val_loss: 2.2161 - val_accuracy: 0.4167\n",
      "Epoch 5/25\n",
      "15/15 [==============================] - 10s 659ms/step - loss: 0.5721 - accuracy: 0.7714 - val_loss: 2.3674 - val_accuracy: 0.4167\n",
      "Epoch 6/25\n",
      "15/15 [==============================] - 9s 575ms/step - loss: 0.5072 - accuracy: 0.7863 - val_loss: 2.8908 - val_accuracy: 0.3056\n",
      "Epoch 7/25\n",
      "15/15 [==============================] - 9s 562ms/step - loss: 0.4715 - accuracy: 0.8013 - val_loss: 3.0875 - val_accuracy: 0.3333\n",
      "Epoch 8/25\n",
      "15/15 [==============================] - 9s 559ms/step - loss: 0.3858 - accuracy: 0.8397 - val_loss: 3.5798 - val_accuracy: 0.5278\n",
      "Epoch 9/25\n",
      "15/15 [==============================] - 8s 518ms/step - loss: 0.3665 - accuracy: 0.8761 - val_loss: 3.3409 - val_accuracy: 0.4722\n",
      "Epoch 10/25\n",
      "15/15 [==============================] - 8s 523ms/step - loss: 0.3958 - accuracy: 0.8355 - val_loss: 2.9799 - val_accuracy: 0.3889\n",
      "Epoch 11/25\n",
      "15/15 [==============================] - 8s 538ms/step - loss: 0.3008 - accuracy: 0.8782 - val_loss: 3.6183 - val_accuracy: 0.4167\n",
      "Epoch 12/25\n",
      "15/15 [==============================] - 8s 522ms/step - loss: 0.2655 - accuracy: 0.8953 - val_loss: 3.7638 - val_accuracy: 0.4167\n",
      "Epoch 13/25\n",
      "15/15 [==============================] - 8s 518ms/step - loss: 0.2136 - accuracy: 0.9252 - val_loss: 5.1357 - val_accuracy: 0.4444\n",
      "Epoch 14/25\n",
      "15/15 [==============================] - 8s 512ms/step - loss: 0.2048 - accuracy: 0.9209 - val_loss: 5.0774 - val_accuracy: 0.4444\n",
      "Epoch 15/25\n",
      "15/15 [==============================] - 8s 504ms/step - loss: 0.1827 - accuracy: 0.9231 - val_loss: 4.3751 - val_accuracy: 0.4167\n",
      "Epoch 16/25\n",
      "15/15 [==============================] - 8s 534ms/step - loss: 0.1355 - accuracy: 0.9551 - val_loss: 4.9510 - val_accuracy: 0.4167\n",
      "Epoch 17/25\n",
      "15/15 [==============================] - 8s 511ms/step - loss: 0.1399 - accuracy: 0.9509 - val_loss: 4.9012 - val_accuracy: 0.3889\n",
      "Epoch 18/25\n",
      "15/15 [==============================] - 8s 519ms/step - loss: 0.2468 - accuracy: 0.9209 - val_loss: 4.9456 - val_accuracy: 0.3889\n",
      "Epoch 19/25\n",
      "15/15 [==============================] - 8s 504ms/step - loss: 0.1489 - accuracy: 0.9573 - val_loss: 5.5090 - val_accuracy: 0.3056\n",
      "Epoch 20/25\n",
      "15/15 [==============================] - 8s 525ms/step - loss: 0.3123 - accuracy: 0.9081 - val_loss: 4.5819 - val_accuracy: 0.3333\n",
      "Epoch 21/25\n",
      "15/15 [==============================] - 8s 512ms/step - loss: 0.1638 - accuracy: 0.9509 - val_loss: 6.1630 - val_accuracy: 0.4722\n",
      "Epoch 22/25\n",
      "15/15 [==============================] - 9s 572ms/step - loss: 0.1131 - accuracy: 0.9551 - val_loss: 5.3689 - val_accuracy: 0.3333\n",
      "Epoch 23/25\n",
      "15/15 [==============================] - 8s 545ms/step - loss: 0.1872 - accuracy: 0.9359 - val_loss: 5.1176 - val_accuracy: 0.2778\n",
      "Epoch 24/25\n",
      "15/15 [==============================] - 8s 542ms/step - loss: 0.1053 - accuracy: 0.9637 - val_loss: 5.6379 - val_accuracy: 0.4722\n",
      "Epoch 25/25\n",
      "15/15 [==============================] - 8s 539ms/step - loss: 0.0414 - accuracy: 0.9872 - val_loss: 5.5476 - val_accuracy: 0.4167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defina os parâmetros\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Crie geradores de dados para treinamento e validação\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Define a fração de validação\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Use 'categorical' para classificação multiclasse\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Crie o modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))  # 4 classes: floresta, deserto, nuvens, água\n",
    "\n",
    "# Compile o modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Treine o modelo\n",
    "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Salve o modelo treinado\n",
    "model.save('image_classification_model.h5')\n",
    "\n",
    "# Para classificar novas imagens, carregue o modelo e use model.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
